# 故障与部分失效

在单节点上开发程序时，通常它应该以一种确定性的方式运行：要么工作，要么出错。单台节点上的软件通常不应该出现模棱两可的现象：当硬件正常工作时，相同的操作通常总会产生相同的结果（即确定性）。然而当涉及多台节点时，情况发生了根本性变化。对于这种分布式系统，理想化的标准正确模型不再适用，我们必须面对 个可能非常混乱的现实。 

在分布式系统中，可能会出现系统的一部分工作正常，但其他某些部分出现难以预测的故障，我们称之为部分失效。问题的难点就在于这种部分失效是不确定的：如果涉及多个节点和网络，几乎肯定会碰到有时网络正常，有时则莫名的失败。正如接下来马上要看到的，通过网络发送消息的延迟非常不确定，有时甚至根本不知道执行是否成功。

正是由于这种不确定性和部分失效大大提高了分布式系统的复杂性。

## 云计算和超算

关于如何构建大规模计算系统有以下几种不同的思路：

1. 规模的一个极端是高性能计算（high-performance computing, HPC）。包含成千上万个 CPU 的超级计算机构成一个庞大的集群，通常用于计算密集型的科学计算任务。
2. 另一个极端是云计算。虽然云计算的定义并非那么明确，但通常它具有以下特征：多租户数据中心，通用计算机，用 IP 以太网链接，弹性/按需资源分配，并
   按需计费。
3. 传统企业数据中心则位于以上两个极端之间。

要使分布式系统可靠工作，就必然面临部分失效，这就需要依靠软件系统来提供容错机制。换句话说，我们需要在不可靠的组件之上构建可靠的系统。 

# 不可靠的网络

互联网以及大多数数据中心的内部网络（通常是以太网）都是==**异步网络**==（*asynchronous packet networks*）。在这种网络中，一个节点可以发送消息（数据包）到另一个节点，但是网络并不保证它什么时候到达，甚至它是否一定到达。发送之后等待响应过程中，有很多事情可能会出错（见图 8-1 所示的例子）：

1. 请求可能已经丢失（比如有人拔掉了网线）。
2. 请求可能正在某个队列中等待，无法马上发送（也许网络或接收方已经超负荷）。
3. 远程接收节点可能已经失效（例如崩溃或关机）。
4. 远程接收节点可能暂时无法响应（例如正在运行长时间的垃圾回收，请参阅本章后面的 “进程暂停”）。
5. 远程接收节点已经完成了请求处理，但回复却在网络中丢失（例如网络交换机配置错误）。
6. 远程接收节点已经完成了请求处理，但回复却被延迟处理（例如网络或者发送者的机器超出负荷）。

![image-20220123213253298](https://littleneko.oss-cn-beijing.aliyuncs.com/img/image-20220123213253298.png)

发送者甚至不清楚数据包是否完成了发送，只能选择让接收者来回复响应消息，但回复也有可能丢失或延迟。这些问题在一个异步网络中无法明确区分，发送者拥有的唯一信息是，尚未收到响应，但却无法判定具体原因。

处理这个问题通常采用==**超时机制**==（*timeout*）：在等待一段时间之后，如果仍然没有收到回复则选择放弃，并且认为响应不会到达。==但是，即使判定超时，仍然并不清楚远程节点是否收到了请求（一种情况，请求仍然在某个地方排队，即使发送者放弃了，但最终请求会发送到接收者）==。

## 超时与无限期的延迟

如果超时是故障检测唯一可行的方法，那么超时应该设多长呢？不幸的是没有标准的答案。

设置较长的超时值意味着更长时间的等待，才能宣告节点失效（在此期间，用户只能等待或者拿到错误信息）。较短的超时设置可以帮助快速检测故障，但可能会出现误判，例如实际上节点只是出现暂时的性能波动（由于节点或网络上的高负载峰值），结果却被错误地宣布为失效。

当一个节点被宣告为失效，其承担的职责要交给到其他节点，这个过程会给其他节点以及网络带来额外负担，特别是如果此时系统已经处于高负荷状态。==例如节点只是负载过高而出现了响应缓慢，转移负载到其他节点可能会导致失效扩散，产生级联扩大效应，在极端情况下，所有节点都宣告对方死亡，造成服务处于事实停止状态。==

设想一个虚拟的系统，其网络可以保证数据包的最大延迟在一定范围内：要么在时间 d 内完成交付，要么丢失。此外，假定一个非故障节点总能够在一段时间 r 内完成请求处理。此时，可以确定成功的请求总能够在 2d+r 时间内收到响应，如果在此时间内没有收到响应，则可以推断网络或者远程节点发生了失效，那么 2d+r 是一个理想的超时设置。

然而事实上绝大多数系统都没有类似的保证：==异步网络理论上的延迟无限大==（即使尽力发送数据包，但数据包到达时间并没有上确界），多数服务端也无法保证在给定的某个时间内一定完成请求处理（参阅本章后面的 “响应时间保证“）。

### 网络拥塞与排队

计算机网络上数据包延的变化根源往往在于排队：

* 当多个不同节点同时发送数据包到相同的目标节点时，网络交换机会出现排队，然后依次将数据包转发到目标网络。
* 当数据包到达目标机器后，如果所有 CPU 核都处于繁忙状态，则网络数据包请求会被操作系统排队，直到应用程序能够处理。
* 在虚拟化环境下，CPU 核会切换虚拟机，从而导致正在运行的操作系统会突然暂停几十毫秒。在这段时间，客户虚机无法从网络中接收任何数据，入向的包会被虚拟机管理器排队缓冲。
* TCP 执行流量控制（也称为拥塞消除，或背压）时，节点会主动限制自己的发送速率以避免加重网络链路或接收节点负载，这意味着数据甚至在进入网络之前就已经在发送方开始了排队。

而且，对于 TCP 如果在某个超时范围内（根据往返时间来推算）没有收到确认，则认为数据包已经丢失进而触发自动重传。虽然这一过程对应用程序透明，但肯定会引入额外的延迟（等待超时到期，等待重传的数据包得到确认）。

## 同步与异步网络

我们将数据中心网络与固定电话网络（非移动蜂窝，非 VoIP）进行对比分析，首先后者非常可靠。这样的固定电话网络需要有持续端到端的低延迟和足够的带宽来传输音频数据。计算机网络能否实现类似的高可靠性和确定性呢？

当通过电话网络拨打电话时，系统会动态建立一条电路：在整个线路上为呼叫分配个固定的、带宽有保证通信链路，该电路一直维持到通话结束。例如，ISDN 网络固定以每秒 4000 帧的速率运行。当新的呼叫建立后，在每个帧（每个方向）内分配 16bit 的空间，在通话期间，每一方都可以保证在 250μs 内完成发送一条 16bit 的音频数据。

这种网络本质是同步的（*synchronous*）：即使数据中间经过了多个路由器，16bit 空间在电路建立时已经在网络中得到预留，不会受到排队的影响。由于没有排队，网络最大的端到端延迟是固定的。我们称之为有界延迟（*bounded delay*）。

### 网络延迟是否可预测？

请注意，固定电话网络中的电路与 TCP 连接存在很大不同：电路方式总是预留固定带宽，在电路建立之后其他入无法使用；而 TCP 连接的数据包则会尝试使用所有可用的网络带宽。TCP 可以传送任意大小可变的数据块（例如电子邮件或网页），它会尽力在最短的时间内完成数据发送。而当 TCP 连接空闲时，通常不占用任何带宽。

假设数据中心网络和互联网基于电路交换网络，一旦电路建立完成则可以保证最大往返时间。然而，事实是以太网和 IP 都是基于分组交换协议，这种协议注定受到排队的影响，从而导致网络延迟不确定，在这些协议里完全没有电路的概念。

那为什么数据中心网络和互联网采用分组交换呢？答案是，它们针对突发流晕进行了很多优化。电路非常适合音频或视频通话，通话期间只需每秒传送固定数量的数据。但对于访问网页，发送电子邮件或传输文件等无法事先确定带宽需求，我们只是希望它尽快完成。

如果你想通过电路链接来传输文件，将不得不预估一个待分配的带宽。如果预估值太低，传输速度就特别缓慢，甚至无法实际可用；如果预估带宽太高，电路甚至无法完成建立（因为如果无法预留所需的带宽，电路就无法建立）。所以，对于突发数据的传输，电路网络无法充分利用网络容量，导致发送缓慢。相比之下，TCP 动态调整传输速率则可以充分利用所有可用的网络容量。

# 不可靠的时钟

在分布式系统中，时间总是件棘手的问题，由于跨节点通信不可能即时完成，消息经由网络从一台机器到另一台机器总是需要花费时间。收到消息的时间应该晚于发送的时间，但是由于网络的不确定延迟，精确测量面临着很多挑战。这些情况使得多节点通信时很难确定事情发生的先后顺序。

而且，网络上的每台机器都有自己的时钟硬件设备，通常是石英晶体振荡器。这些设备并非绝对准确，即每台机器都维护自己本地的时间版本，可能比其他机器稍快或更慢。可以在一定程度上同步机器之间的时钟，最常用的方法是网络时间协议（Network Time Protocol, NTP），它可以根据一组专门的时间服务器来调整本地时间，时间服务器则从精确更高的时间源（如 GPS 接收机）获取高精度时间。

## 单调时钟与墙上时钟⭐

### 墙上时钟 (Time-of-day clocks)

墙上时钟根据某个日历（也称为墙上时间）返回当前的日期与时间。例如，Linux 的 `clock_gettime(CLOCK_REALTIME)` 和 Java 中的 `System.currentTimeMillis()` 会返回自纪元 1970 年 1 月 1 日（UTC）以来的秒数和毫秒数，不含闰秒。而有些系统则使用其他日期作为参考点。

墙上时钟可以与 NTP 同步。但是，如下一节所述，这里还存在一些奇怪问题。特别是，如果本地时钟远远快于 NTP 服务器，强行重置之后会跳回到先前的某个时间点。这种跳跃以及经常忽略闰秒，导致其不太适合测量时间间隔。

### 单调时钟 (Monotonic clocks)

单调时钟更适合测晕持续时间段（时间间隔），例如超时或服务的响应时间：Linux上的 `clock_gettime(CLOCK_MONOTONIC)` 和 Java 中的 `System.nanoTime()`  返回的即是单调时钟。单调时钟的名字来源于它们保证总是向前（而不会出现墙上时钟的回拨现象）。

可以在一个时间点读取单调时钟的值，完成某项工作，然后再次检查时钟。时钟值之间的差值即两次检查之间的时间间隔。注意，==单调时钟的绝对值并没有任何意义==，它可能电脑启动以后经历的纳秒数或者其他含义。因此比较不同节点上的单调时钟值毫无意义，它们没有任何相同的基准。

如果服务器有多路 CPU，则每个 CPU 可能有单独的计时器，且不与其他 CPU 进行同步。由于应用程序的线程可能会调度到不同的 CPU 上，此时，操作系统会补偿多个计时器之间的偏差，从而为应用层提供统一的单调递增计时。不过最好还是对这种偏差补偿持谨慎态度。

如果 NTP 检测到本地石英比时间服务器上更快或者更慢，NTP 会调整本地石英的震动频率（这被称为摆动）。默认情况下，NTP 允许速率加快或减慢的最大幅度为 0.05%，但 NTP 并不会直接调整单调时钟向前或者回拨。单调时钟的精度通常很高，在如今大多数系统中，可以测量几微秒或更短的时间间隔。

## 时钟同步与准确性

单调时钟不需要同步，但是墙上时钟需要根据 NTP 服务器或其他外部时间源做必要的调整。然而，我们获取时钟的方法并非预想那样可靠或准确，硬件时钟和NTP 可能会出现一些莫名其妙的现象。举几个例子：

* 计算机中的石英钟不够精确，存在漂移现象（运行速度会加快或减慢）。时钟漂移主要取决于机器的温度。谷歌假设其服务器的时钟偏移为 200 ppm（百万分之一），相当于如果每 30 秒与服务器重新同步一次，则可能出现的最大偏差为 6 毫秒（注：前后各 3 毫秒），或者每天一次同步，则最大偏差为 17 秒。问题也限制了可以达到的最佳精度。
* 如果时钟与 NTP 服务器的时钟差别太大，可能会出现拒绝同步，或者本地时钟将被强制重置。在重置前后应用程序观察可能会看到时间突然倒退或突然跳跃
  的现象。
* NTP 同步会受限于当时的网络环境特别是延迟，如果网络拥塞、数据包延迟变化不定，则 NTP 同步的准确性会受影响。 
* 闰秒会产生一分钟为 59 秒或 61 秒的现象，这会在使一些对闰秒毫无防范的系统出现混乱。
* 在虚拟机中，由于硬件时钟也是被虚拟化的，这对于需要精确计时的应用程序提出了额外的挑战。当虚拟机共享一个CPU核时，每个虚拟机会出现数十亳秒
  内的暂停以便切换客户虚机。但从应用的角度来看，这种停顿会表现为时钟突然向前发生了跳跃。

高精度的时钟可以采用 GPS 接收机，精确时间协议（PTP）并辅以细致的部署和监测。但通常也意味着大量的资源投入和技术门槛，并持续监控时钟同步可能出现的错误情况。

## 依赖同步的时钟

### 时间戳与事件顺序⭐

对于一个常见的功能：跨节点的事件排序，如果它高度依赖时钟计时，就存在一定的技术风险。例如，两个客户端同时写入分布式数据库，谁先到达？哪一个操作是最新的呢？

图 8-3 给出了这样的危险例子，即多主节点复制的分布式数据库高度依赖于墙上时钟（图 5-9 是另一个类似的例子）。客户端 A 在节点 1 上写入 x=1，写入被复制到节点 3；客户端 B 在节点 3 上增加 x（现在 x = 2)。最后，这两个写入都被复制到节点 2 上。

在图 8-3中，写入被复制到其他节点时，会根据源写入节点上的墙上时钟来标记时间戳。在该例子中，时钟同步机制稳定工作，节点 1 和节点 3 之间的时钟偏差小于 3ms，这比实践中的多数情况可能都要更好。

![image-20220123222159727](https://littleneko.oss-cn-beijing.aliyuncs.com/img/image-20220123222159727.png)

但是，这样的时间戳却不能正确排序事件：写入 x = 1 的时间戳为 42.004 秒，写入 x = 2 虽然后续发生，时间戳却是 42.003s。当节点 2 收到这两个事件时，会根据时间戳错误地判断 x = 1 是最新值，然后决定丢弃 x = 2，就给就导致客户端 B 的增量操作丢失。

这种冲突解决策略被称为最后写人获胜（LWW），在多主节点以及无主节点复制数据库（如 Cassandra 和 Riak）中广泛使用（参阅第 5 章 “最后写入获胜”）。

因此，通过保持 “最新” 值并丢弃其他值来解决冲突看似不错，但要注意，最新的定义如果取决于墙上时钟就会引入偏差。即使采用了 NTP 同步时钟，依然可能会出现在时间戳 100 毫秒时（根据发送者的时年中）发送了某个数据包，却在时间戳 99 毫秒（根据接收者的时钟）到达，这看起来好像是数据包还没发送就先到达了。

那么 NTP 时钟同步能否做到极高的精度从而避免这种错误的顺序间题呢？很难做到。因为除了石英漂移等误差来源之外，NTP 同步精度本身要受限于所在的网络延迟。要达到正确的排序，需要时钟源精度要远远高于被测量的对象（即网络延迟）

==对于排序来讲，基于递增计数器而不是振荡石英晶体的逻辑时钟（*logical clocks*）是更可靠的方式==（参见第 5 章 ”检测并发写"）。逻辑时钟并不测量一天的某个时间点或时间间隔，而是事件的相对顺序（事件发生的相对前后关系）。与之对应的，墙上时钟和单调时钟都属于物理时钟。 

### 时钟的置信区间⭐

或许墙上时钟会返回微秒甚至纳秒级别的信息，但是这种精度的测晕值其实并不可信。因此，==我们不应该将时钟读数视为一个精确的时间点，而更应该视为带有置信区间的时间范围==。例如，系统可能有 95% 的置信度认为目前时间介于 10.3 ~10.5 秒之间。如果我们可完全相信的精度为 +/-100 毫秒，那么时间戳中那些微秒级的读数并无实际意义。

可惜大多数系统并不提供这种误差查询接口。 

这里有趣的是 Google Spanner 中的 *TrueTime* API 它会明确地报告本地时钟的置信区间。当查询当前时间时，你会得到两个值：[*earliest*, *latest*] 分别代表误差的最大偏差范围。基于上述两个时间戳，可以知道实际时间应在其范围之内。该间隔的范围主要取决于本地石英钟最后与高精时钟源同步后所经历的时间长短。

### 全局快照的同步时钟⭐

在第 7 章 “快照隔离与可重复读” 中，我们介绍了快照隔离。常见的快照隔离实现中需要单调递增事务 ID。如果写入发生在快照之后（即写入具有比快照更大的事务 ID），那么该写入对于快照不可见。在单节点数据库上，一个简单的计数器足以生成事务 ID。

但是，当数据库分布在多台机器上（可能跨越多个数据中心）时，由于需要复杂的协调以产生全局的、单调递增的事务 ID（跨所有分区）。事务 ID 要求必须反映因果关系：事务 B 如果要读取事务 A 写入的值，则 B 的事务 ID 必须大于 A 的事务 ID，否则快照将不一致。考虑到大量、频繁的小数据包，在分布式系统中创建事务 ID 通常会引入瓶颈。

> 还有一种分布式序列号生成器，例如 Twitter 的 Snowflake，它用更为扩展的方式（例如将 ID 空间划分不同的范围，然后分配给不同的节点）来生成近似单调递增的唯一ID。但通常无法保证与因果关系一致的顺序。具体参阅笫 9 章的 “顺序保证 ”。

Google Spanner 采用以下思路来实现跨数据中心的快照隔离。它根据 TrueTime API 返回的时钟置信区间，并基于以下观察结果：==如果有两个置信区间，每个置信区间都包含最早和最新可能的时间戳（A = [A~earliest~ , A~latesr~] 和 B = [B~earliest~ , B~larest~]），且这两个区间没有重叠，那么可以断定 B一定发生在 A 之后。只有发生了重叠，A 和 B 发生顺序才无法明确。==

==为了确保事务时间戳反映因果关系，Spanner 在提交读写事务之前故意等待置信区间的长度==。这样做的目的是，确保所有读事务要足够晚才发生，避免与先前的事务的置信区间产生重叠。而为了尽量缩短潜在的等待时间，Spanner 需要使时钟的误差范围尽可能的小，为此，Google 在每个数据中心都部署了一个 GPS 接收器或原子钟，保证所有时钟同步在约 7 ms 之内完成。

## 进程暂停⭐

另一个分布式系统中危险使用时钟的例子：假设数据库每个分区只有一个主节点，只有主节点可以接受写入。那么其他节点该如何确信该主节点没有被宣告失效，可以安全地写入呢？

一种思路是主节点从其他节点获得一个==**租约**==（*lease*），类似一个带有超时的锁。某一个时间只有一个节点可以拿到租约，某节点获得租约之后，在租约到期之前，它就是这段时间内的主节点。为了维持主节点的身份，节点必须在到期之前定期去更新租约。如果节点发生了故障，则续约失败，这样另一个节点到期之后就可以接管。

典型处理流程如下所示：

```java
while (true) {
	request = getIncomingRequest();
	// Ensure that the lease always has at least 10 seconds remaining
	if (lease.expiryTimeMillis - System.currentTimeMillis() < 10000) {
		lease = lease.renew();
	}
	if (lease.isValid()) {
		process(request);
	}
}
```

这代码有什么问题么？首先，它依赖于同步的时钟：租约到期时间由另一台机器所设置（例如，另一台机器的当前时间加上 30 秒得到到期时间），并和本地时钟进行比较。如果时钟之间有超过几秒的差异，这段代码会出现些奇怪的事情。

其次，即使我们改为仅使用本地单调时钟，还有另一个问题：代码假定时间检查点 `System.currentTimeMillis()` 与请求处理 `process(request)` 间隔很短，通常代码运行足够快，所以设置 10 秒的缓冲区来确保在请求处理过程中租约不会过期。

==但是，如果程序执行中出现了某些意外的暂停呢？例如，假设线程在 lease.isValid() 消耗了整 15 秒。那么当开始处理请求时，租约已经过期，另一个节点已经接管了主节点。可惜我们无法有效通知线程暂停了这么长时间了，后续代码也不会注意到租约已经到期，除非运行到下一个循环迭代。==不过，到那个时候它已经做了一些不安全的请求处理。

那么，一个线程可能会暂停这么长时间么？这是可能的，发生这种情况的原因有很多种：

* GC
* 在虚拟化环境中，可能会暂停虚拟机（暂停所有执行进程并将内存状态保存到磁盘）然后继续（从内存中加载数据然后继续执行）
* 当操作系统执行线程上下文切换时，或者虚拟机管理程序切换到另一个虚机时，正在运行的线程可能会在代码的任意位置被暂停。在虚拟机环境中，这种被其他虚拟机中断的 CPU 时间称为窃取时间。如果机器负载很高（即等待运行的线程很长），被暂停的线程可能需要一段时间之后才能再次运行。
* 如果应用程序执行同步磁盘操作，则线程可能暂停并等待磁盘 I/O 完成。
* 如果操作系统配置了基于磁盘的内存交换分区，内存访问可能触发缺页中断，进而需要从磁盘中加载内存页。

分布式系统中的一个节点必须假定，执行过程中的任何时刻都可能被暂停相当长一段时间，包括运行在某个函数中间。暂停期间，整个集群的其他部分都在照常运行，甚至会一致将暂停的节点宣告为故障节点。最终，暂停的节点可能会回来继续运行，除非再次检查时钟，否则它对刚刚过去的暂停毫无意识。

### 响应时间保证

某些软件如果在指定时间内无法响应则会导致严重后果，这些运行环境包括：飞机、火箭、机器人 、汽车和其他需要对输入传感器快速做出响应的组件等。对于这些系统，软件有一个必须做出响应的上限：如果无法满足，会导致系统级故障，这就是所谓的==**硬实时系统**==（*hard real-time*）。

提供实时保证需要来自软件栈的多个层面的支持：首先是一个实时操作系统（real­ time operating system, RTOS），保证进程在给定的间隔内完成 CPU 时间片的调度分配；其次，库函数也必须考虑最坏的执行时间；然后，动态内存分配很可能要受限或者完全被禁止（如果存在实时垃圾收集器，确保 GC 不能处理太多任务）；最终还是需要大量、充分的测试和验证，以确保满足要求。

### 调整垃圾回收的影响

略